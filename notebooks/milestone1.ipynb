{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.9.2\n",
      "  latest version: 4.10.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://test.pypi.org/simple/, https://pypi.org/simple\n",
      "Requirement already satisfied: nicenumber in /opt/miniconda3/envs/525/lib/python3.8/site-packages (0.2.7)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.2.3 in /opt/miniconda3/envs/525/lib/python3.8/site-packages (from nicenumber) (1.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/miniconda3/envs/525/lib/python3.8/site-packages (from pandas<2.0.0,>=1.2.3->nicenumber) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/miniconda3/envs/525/lib/python3.8/site-packages (from pandas<2.0.0,>=1.2.3->nicenumber) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/miniconda3/envs/525/lib/python3.8/site-packages (from pandas<2.0.0,>=1.2.3->nicenumber) (1.20.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/525/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas<2.0.0,>=1.2.3->nicenumber) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/envs/525/lib/python3.8/site-packages (4.59.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installations\n",
    "import sys\n",
    "%conda install --yes --prefix {sys.prefix} seaborn\n",
    "\n",
    "# our package from 524, used to pretty-print file sizes during download. Not super necessary but fun to use.\n",
    "%pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple nicenumber\n",
    "\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import download as dl\n",
    "from src import functions as f\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import rpy2.rinterface\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# install the packages https://arrow.apache.org/docs/python/install.html\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# How to install put instructions https://anaconda.org/conda-forge/rpy2\n",
    "import rpy2.rinterface\n",
    "\n",
    "# install this https://pypi.org/project/rpy2-arrow/#description  pip install rpy2-arrow\n",
    "# have to install this as well conda install -c conda-forge r-arrow \n",
    "import rpy2_arrow.pyarrow_rarrow as pyra\n",
    "\n",
    "import pyarrow.feather as feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext rpy2.ipython\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO    58   src.download               Downloading 814MB file in 10MB chunks.\n",
      "100%|██████████| 814M/814M [04:19<00:00, 3.14MiB/s]\n",
      "INFO    69   src.download               File downloaded to: /Users/Jayme/OneDrive/MDS/525/rainfall_group22/data/data.zip\n",
      "INFO    126  src.download               Unpacking zip to: /Users/Jayme/OneDrive/MDS/525/rainfall_group22/data/csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 936.94 MiB, increment: 660.67 MiB\n",
      "CPU times: user 15.8 s, sys: 7.58 s, total: 23.4 s\n",
      "Wall time: 4min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "# download and unzip data files\n",
    "files = dl.download_files('data.zip', chunk_size=10)\n",
    "\n",
    "if files:\n",
    "    dl.unzip(p=files[0], p_dst='csv', delete=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Combine CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set download directories\n",
    "p_data = dl.p_data # top level data dir\n",
    "p_csv = p_data / 'csv' # sub dir for saving loose csvs\n",
    "p_combined = p_data / 'rainfall.csv' # main csv file to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 13064.04 MiB, increment: 12683.50 MiB\n",
      "CPU times: user 54.9 s, sys: 6.91 s, total: 1min 1s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "# combine csvs with pandas\n",
    "csvs = [p for p in p_csv.glob('*.csv')]\n",
    "dfs = []\n",
    "\n",
    "# load individual dfs and save to list\n",
    "for p in csvs:\n",
    "    model_name = p.name.split('_')[0]\n",
    "\n",
    "    df = pd.read_csv(p) \\\n",
    "        .assign(model=model_name)\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "# concat all dfs\n",
    "df = pd.concat(dfs) \\\n",
    "    .rename(columns={'rain (mm/day)': 'rain'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runtimes\n",
    "\n",
    "Times to combine dataframe csvs for each team member:\n",
    "\n",
    "|User|OS|Processor|RAM|Load Time|\n",
    "|:--|:--|:--|:--|:--|\n",
    "|Jayme|Mac OS|2.4 GHz 8-Core Intel Core i9|32 GB 2667 MHz DDR4| 62s|\n",
    "|Zhiyong| | | |\n",
    "|Marc| | | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62513863, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>lat_min</th>\n      <th>lat_max</th>\n      <th>lon_min</th>\n      <th>lon_max</th>\n      <th>rain</th>\n      <th>model</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1889-01-01 12:00:00</td>\n      <td>-35.439867</td>\n      <td>-33.574619</td>\n      <td>141.5625</td>\n      <td>143.4375</td>\n      <td>4.244226e-13</td>\n      <td>MPI-ESM-1-2-HAM</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1889-01-02 12:00:00</td>\n      <td>-35.439867</td>\n      <td>-33.574619</td>\n      <td>141.5625</td>\n      <td>143.4375</td>\n      <td>4.217326e-13</td>\n      <td>MPI-ESM-1-2-HAM</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1889-01-03 12:00:00</td>\n      <td>-35.439867</td>\n      <td>-33.574619</td>\n      <td>141.5625</td>\n      <td>143.4375</td>\n      <td>4.498125e-13</td>\n      <td>MPI-ESM-1-2-HAM</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1889-01-04 12:00:00</td>\n      <td>-35.439867</td>\n      <td>-33.574619</td>\n      <td>141.5625</td>\n      <td>143.4375</td>\n      <td>4.251282e-13</td>\n      <td>MPI-ESM-1-2-HAM</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1889-01-05 12:00:00</td>\n      <td>-35.439867</td>\n      <td>-33.574619</td>\n      <td>141.5625</td>\n      <td>143.4375</td>\n      <td>4.270161e-13</td>\n      <td>MPI-ESM-1-2-HAM</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                  time    lat_min    lat_max   lon_min   lon_max  \\\n0  1889-01-01 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n1  1889-01-02 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n2  1889-01-03 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n3  1889-01-04 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n4  1889-01-05 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n\n           rain            model  \n0  4.244226e-13  MPI-ESM-1-2-HAM  \n1  4.217326e-13  MPI-ESM-1-2-HAM  \n2  4.498125e-13  MPI-ESM-1-2-HAM  \n3  4.251282e-13  MPI-ESM-1-2-HAM  \n4  4.270161e-13  MPI-ESM-1-2-HAM  "
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 62513863 entries, 0 to 3541152\n",
      "Data columns (total 7 columns):\n",
      " #   Column   Dtype  \n",
      "---  ------   -----  \n",
      " 0   time     object \n",
      " 1   lat_min  float64\n",
      " 2   lat_max  float64\n",
      " 3   lon_min  float64\n",
      " 4   lon_max  float64\n",
      " 5   rain     float64\n",
      " 6   model    object \n",
      "dtypes: float64(5), object(2)\n",
      "memory usage: 3.7+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save combined data back to csv\n",
    "df.to_csv(p_combined, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.6G\tdata/rainfall.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "du -sh data/rainfall.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 16101.11 MiB, increment: 0.73 MiB\n",
      "CPU times: user 79.4 ms, sys: 66.1 ms, total: 145 ms\n",
      "Wall time: 1.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# Load csv with dask\n",
    "\n",
    "ddf = dd.read_csv(p_combined, assume_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>lat_min</th>\n      <th>lat_max</th>\n      <th>lon_min</th>\n      <th>lon_max</th>\n      <th>rain</th>\n      <th>model</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1889-01-01 12:00:00</td>\n      <td>-35.439867</td>\n      <td>-33.574619</td>\n      <td>141.5625</td>\n      <td>143.4375</td>\n      <td>4.244226e-13</td>\n      <td>MPI-ESM-1-2-HAM</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1889-01-02 12:00:00</td>\n      <td>-35.439867</td>\n      <td>-33.574619</td>\n      <td>141.5625</td>\n      <td>143.4375</td>\n      <td>4.217326e-13</td>\n      <td>MPI-ESM-1-2-HAM</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1889-01-03 12:00:00</td>\n      <td>-35.439867</td>\n      <td>-33.574619</td>\n      <td>141.5625</td>\n      <td>143.4375</td>\n      <td>4.498125e-13</td>\n      <td>MPI-ESM-1-2-HAM</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1889-01-04 12:00:00</td>\n      <td>-35.439867</td>\n      <td>-33.574619</td>\n      <td>141.5625</td>\n      <td>143.4375</td>\n      <td>4.251282e-13</td>\n      <td>MPI-ESM-1-2-HAM</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1889-01-05 12:00:00</td>\n      <td>-35.439867</td>\n      <td>-33.574619</td>\n      <td>141.5625</td>\n      <td>143.4375</td>\n      <td>4.270161e-13</td>\n      <td>MPI-ESM-1-2-HAM</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                  time    lat_min    lat_max   lon_min   lon_max  \\\n0  1889-01-01 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n1  1889-01-02 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n2  1889-01-03 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n3  1889-01-04 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n4  1889-01-05 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n\n           rain            model  \n0  4.244226e-13  MPI-ESM-1-2-HAM  \n1  4.217326e-13  MPI-ESM-1-2-HAM  \n2  4.498125e-13  MPI-ESM-1-2-HAM  \n3  4.251282e-13  MPI-ESM-1-2-HAM  \n4  4.270161e-13  MPI-ESM-1-2-HAM  "
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Python EDA\n",
    "\n",
    "Here we will investigate and summarize the following approaches to reduce memory usage while performing a simple EDA:\n",
    "\n",
    "1. Baseline\n",
    "2. Load data in chunks\n",
    "3. Load only columns of interest\n",
    "4. Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Baseline\n",
    "\n",
    "Naive approach, read all columns with pandas `read_csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_max_rain = lambda x: print(f'Max rainfall: {x:.2f} mm/day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max rainfall: 432.94 mm/day\n",
      "peak memory: 14973.23 MiB, increment: 2598.79 MiB\n",
      "CPU times: user 54.6 s, sys: 4.27 s, total: 58.9 s\n",
      "Wall time: 59.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "max_rain_baseline = pd.read_csv(p_combined).rain.max()\n",
    "print_max_rain(max_rain_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load data in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max rainfall: 432.94 mm/day\n",
      "peak memory: 1322.71 MiB, increment: 1035.25 MiB\n",
      "CPU times: user 52.1 s, sys: 2.77 s, total: 54.8 s\n",
      "Wall time: 55.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "max_rain_chunks = np.finfo('float64').min\n",
    "\n",
    "for df_chunk in pd.read_csv(p_combined, chunksize=1_000_000):\n",
    "    cur_max = df_chunk.rain.max()\n",
    "    if cur_max > max_rain_chunks:\n",
    "        max_rain_chunks = cur_max\n",
    "\n",
    "print_max_rain(max_rain_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Load only columns of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max rainfall: 432.94 mm/day\n",
      "peak memory: 5452.88 MiB, increment: 890.59 MiB\n",
      "CPU times: user 25.8 s, sys: 1.14 s, total: 27 s\n",
      "Wall time: 27.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "df_one_col = pd.read_csv(p_combined, usecols=['rain'])\n",
    "max_rain_one = df_one_col.rain.max()\n",
    "print_max_rain(max_rain_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max rainfall: 432.94 mm/day\n",
      "peak memory: 5555.89 MiB, increment: 920.82 MiB\n",
      "CPU times: user 1min 15s, sys: 12.1 s, total: 1min 28s\n",
      "Wall time: 24.8 s\n",
      "Max rainfall: 432.94 mm/day\n",
      "peak memory: 5518.82 MiB, increment: 995.95 MiB\n",
      "CPU times: user 1min 16s, sys: 12.5 s, total: 1min 28s\n",
      "Wall time: 25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "ddf = dd.read_csv(p_combined)\n",
    "max_rain_dask = ddf.rain.max().compute()\n",
    "\n",
    "print_max_rain(max_rain_dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "The following table summarizes memory usage while loading a csv and performing a simple EDA (find maximum rainfall):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<style  type=\"text/css\" >\n#T_638e3_row0_col0,#T_638e3_row0_col1{\n            background-color:  #da3b46;\n            color:  #f1f1f1;\n        }#T_638e3_row1_col0,#T_638e3_row3_col1{\n            background-color:  #417ca8;\n            color:  #000000;\n        }#T_638e3_row1_col1{\n            background-color:  #e7999e;\n            color:  #000000;\n        }#T_638e3_row2_col0{\n            background-color:  #afc6d7;\n            color:  #000000;\n        }#T_638e3_row2_col1{\n            background-color:  #5288af;\n            color:  #000000;\n        }#T_638e3_row3_col0{\n            background-color:  #b1c7d7;\n            color:  #000000;\n        }</style><table id=\"T_638e3_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Peak Memory Usage (MB)</th>        <th class=\"col_heading level0 col1\" >Execution Time (S)</th>    </tr>    <tr>        <th class=\"index_name level0\" >Method</th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>    </tr></thead><tbody>\n                <tr>\n                        <th id=\"T_638e3_level0_row0\" class=\"row_heading level0 row0\" >baseline</th>\n                        <td id=\"T_638e3_row0_col0\" class=\"data row0 col0\" >14973</td>\n                        <td id=\"T_638e3_row0_col1\" class=\"data row0 col1\" >65</td>\n            </tr>\n            <tr>\n                        <th id=\"T_638e3_level0_row1\" class=\"row_heading level0 row1\" >chunks</th>\n                        <td id=\"T_638e3_row1_col0\" class=\"data row1 col0\" >1323</td>\n                        <td id=\"T_638e3_row1_col1\" class=\"data row1 col1\" >55</td>\n            </tr>\n            <tr>\n                        <th id=\"T_638e3_level0_row2\" class=\"row_heading level0 row2\" >single_column</th>\n                        <td id=\"T_638e3_row2_col0\" class=\"data row2 col0\" >5452</td>\n                        <td id=\"T_638e3_row2_col1\" class=\"data row2 col1\" >27</td>\n            </tr>\n            <tr>\n                        <th id=\"T_638e3_level0_row3\" class=\"row_heading level0 row3\" >dask</th>\n                        <td id=\"T_638e3_row3_col0\" class=\"data row3 col0\" >5529</td>\n                        <td id=\"T_638e3_row3_col1\" class=\"data row3 col1\" >25</td>\n            </tr>\n    </tbody></table>",
      "text/plain": "<pandas.io.formats.style.Styler at 0x7f7fc041a790>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_results = dict(\n",
    "    baseline=[14973, 65],\n",
    "    chunks=[1323, 55],\n",
    "    single_column=[5452, 27],\n",
    "    dask=[5529, 25])\n",
    "\n",
    "pd.DataFrame \\\n",
    "    .from_dict(\n",
    "        m_results,\n",
    "        orient='index',\n",
    "        columns=['Peak Memory Usage (MB)', 'Execution Time (S)']) \\\n",
    "    .rename_axis('Method') \\\n",
    "    .style.pipe(f.bg, rev=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "- To find the maximum rainfall, in this case we only needed one column in the table, therefor loading all columns was redundant.\n",
    "- Loading data in chunks reduced our execution time slightly, and greatly reduced peak memory usage.\n",
    "- Both loading only a single column and Dask had similar memory usage, with Dask executing slightly faster (25s).\n",
    "- Overall Dask reduced our memory usage by ~1/3 and execution time by ~60%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data to multiple formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ds.dataset(p_combined, format='csv')\n",
    "arrow_table = dataset.to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 3242.37 MiB, increment: 2930.09 MiB\n",
      "CPU times: user 5.07 s, sys: 8.96 s, total: 14 s\n",
      "Wall time: 8.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "feather.write_feather(arrow_table, 'figshare/combined_data.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 4656.05 MiB, increment: 1407.00 MiB\n",
      "CPU times: user 23.5 s, sys: 20.9 s, total: 44.4 s\n",
      "Wall time: 51.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "pq.write_to_dataset(arrow_table, 'figshare/combined_data.parquet', partition_cols=['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. R EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n",
      "[1] 432.9395\n",
      "Time difference of 26.65761 secs\n",
      "CPU times: user 10 s, sys: 19.2 s, total: 29.3 s\n",
      "Wall time: 26.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "\n",
    "library(arrow)\n",
    "library(dplyr)\n",
    "\n",
    "# Read feather file from python\n",
    "start_time <- Sys.time()\n",
    "r_table <- arrow::read_feather(\"figshare/combined_data.feather\")\n",
    "print(class(r_table))\n",
    "result <- r_table %>% select('rain (mm/day)') %>% max()\n",
    "end_time <- Sys.time()\n",
    "print(result)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "- Our team use `Feather file` approach to transfer the dataframe from python to R\n",
    "- Our EDA(finding the maximum rain drop) only need a single column, so basically in Python we use Python Arrow to explore the data both for the space and time efficiency\n",
    "- Feather is how we store the arrow table in memory to disk\n",
    "- R has a good support on reading feather format data "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('525': conda)",
   "name": "python386jvsc74a57bd0853d569a9b7da01add2e2d9599a0bacc0dbbc936a6f6feab1c78827b3cfdc6dd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "853d569a9b7da01add2e2d9599a0bacc0dbbc936a6f6feab1c78827b3cfdc6dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}