{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rainfall Analysis - 525 Group 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "For easiest readability, please view the notebook as a webpage [here](https://ubc-mds.github.io/rainfall_group22/), or download and open the raw [html](https://github.com/UBC-MDS/rainfall_group22/blob/main/notebooks/milestone1.html).\n",
    "\n",
    "This notebook is to be run in the 525 conda environment. I experienced some issues with `r2py`, but was able to solve them by adding `python=3.8.6` to the yaml file and recreate the environment.\n",
    "\n",
    "We have used some helper functions, so before running please make sure to clone the repo: [https://github.com/UBC-MDS/rainfall_group22](https://github.com/UBC-MDS/rainfall_group22).\n",
    "\n",
    "Below we will install some extra dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.9.2\n",
      "  latest version: 4.10.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://test.pypi.org/simple/, https://pypi.org/simple\n",
      "Requirement already satisfied: nicenumber in /opt/miniconda3/envs/525/lib/python3.8/site-packages (0.2.7)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.2.3 in /opt/miniconda3/envs/525/lib/python3.8/site-packages (from nicenumber) (1.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/miniconda3/envs/525/lib/python3.8/site-packages (from pandas<2.0.0,>=1.2.3->nicenumber) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/miniconda3/envs/525/lib/python3.8/site-packages (from pandas<2.0.0,>=1.2.3->nicenumber) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/miniconda3/envs/525/lib/python3.8/site-packages (from pandas<2.0.0,>=1.2.3->nicenumber) (1.20.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/525/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas<2.0.0,>=1.2.3->nicenumber) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/envs/525/lib/python3.8/site-packages (4.59.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installations\n",
    "import sys\n",
    "\n",
    "# used for pretty printing summary table colours\n",
    "%conda install --yes --prefix {sys.prefix} seaborn\n",
    "\n",
    "# our package from 524, used to pretty-print file sizes during download. Not super necessary but fun to use.\n",
    "%pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple nicenumber\n",
    "\n",
    "# used to show file download progress bar\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import download as dl\n",
    "from src import functions as f\n",
    "from nicenumber import nicenumber as nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "import rpy2.rinterface\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# install the packages https://arrow.apache.org/docs/python/install.html\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# How to install put instructions https://anaconda.org/conda-forge/rpy2\n",
    "import rpy2.rinterface\n",
    "\n",
    "# install this https://pypi.org/project/rpy2-arrow/#description  pip install rpy2-arrow\n",
    "# have to install this as well conda install -c conda-forge r-arrow \n",
    "import rpy2_arrow.pyarrow_rarrow as pyra\n",
    "\n",
    "import pyarrow.feather as feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext rpy2.ipython\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO    58   src.download               Downloading 814MB file in 10MB chunks.\n",
      "100%|██████████| 814M/814M [04:19<00:00, 3.14MiB/s]\n",
      "INFO    69   src.download               File downloaded to: /Users/Jayme/OneDrive/MDS/525/rainfall_group22/data/data.zip\n",
      "INFO    126  src.download               Unpacking zip to: /Users/Jayme/OneDrive/MDS/525/rainfall_group22/data/csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 936.94 MiB, increment: 660.67 MiB\n",
      "CPU times: user 15.8 s, sys: 7.58 s, total: 23.4 s\n",
      "Wall time: 4min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "# download and unzip data files\n",
    "files = dl.download_files('data.zip', chunk_size=10)\n",
    "\n",
    "if files:\n",
    "    dl.unzip(p=files[0], p_dst='csv', delete=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Combine CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set download directories\n",
    "p_data = dl.p_data # top level data dir\n",
    "p_csv = p_data / 'csv' # sub dir for saving loose csvs\n",
    "p_combined = p_data / 'rainfall.csv' # main csv file to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 13064.04 MiB, increment: 12683.50 MiB\n",
      "CPU times: user 54.9 s, sys: 6.91 s, total: 1min 1s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "# combine csvs with pandas\n",
    "csvs = [p for p in p_csv.glob('*.csv')]\n",
    "dfs = []\n",
    "\n",
    "# load individual dfs and save to list\n",
    "for p in csvs:\n",
    "    model_name = p.name.split('_')[0]\n",
    "\n",
    "    df = pd.read_csv(p) \\\n",
    "        .assign(model=model_name)\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "# concat all dfs\n",
    "df = pd.concat(dfs) \\\n",
    "    .rename(columns={'rain (mm/day)': 'rain'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runtimes\n",
    "\n",
    "Times to combine dataframe csvs for each team member:\n",
    "\n",
    "|User|OS|Processor|RAM|Load Time (s)|\n",
    "|:--|:--|:--|:--|--:|\n",
    "|Jayme|Mac OS|2.4 GHz 8-Core Intel Core i9|32 GB 2667 MHz DDR4| 62|\n",
    "|Zhiyong| | | |\n",
    "|Marc| | | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62513863, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>lat_min</th>\n      <th>lat_max</th>\n      <th>lon_min</th>\n      <th>lon_max</th>\n      <th>rain</th>\n      <th>model</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1889-01-01 12:00:00</td>\n      <td>-35.439867</td>\n      <td>-33.574619</td>\n      <td>141.5625</td>\n      <td>143.4375</td>\n      <td>4.244226e-13</td>\n      <td>MPI-ESM-1-2-HAM</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1889-01-02 12:00:00</td>\n      <td>-35.439867</td>\n      <td>-33.574619</td>\n      <td>141.5625</td>\n      <td>143.4375</td>\n      <td>4.217326e-13</td>\n      <td>MPI-ESM-1-2-HAM</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1889-01-03 12:00:00</td>\n      <td>-35.439867</td>\n      <td>-33.574619</td>\n      <td>141.5625</td>\n      <td>143.4375</td>\n      <td>4.498125e-13</td>\n      <td>MPI-ESM-1-2-HAM</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1889-01-04 12:00:00</td>\n      <td>-35.439867</td>\n      <td>-33.574619</td>\n      <td>141.5625</td>\n      <td>143.4375</td>\n      <td>4.251282e-13</td>\n      <td>MPI-ESM-1-2-HAM</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1889-01-05 12:00:00</td>\n      <td>-35.439867</td>\n      <td>-33.574619</td>\n      <td>141.5625</td>\n      <td>143.4375</td>\n      <td>4.270161e-13</td>\n      <td>MPI-ESM-1-2-HAM</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                  time    lat_min    lat_max   lon_min   lon_max  \\\n0  1889-01-01 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n1  1889-01-02 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n2  1889-01-03 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n3  1889-01-04 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n4  1889-01-05 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n\n           rain            model  \n0  4.244226e-13  MPI-ESM-1-2-HAM  \n1  4.217326e-13  MPI-ESM-1-2-HAM  \n2  4.498125e-13  MPI-ESM-1-2-HAM  \n3  4.251282e-13  MPI-ESM-1-2-HAM  \n4  4.270161e-13  MPI-ESM-1-2-HAM  "
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 62513863 entries, 0 to 3541152\n",
      "Data columns (total 7 columns):\n",
      " #   Column   Dtype  \n",
      "---  ------   -----  \n",
      " 0   time     object \n",
      " 1   lat_min  float64\n",
      " 2   lat_max  float64\n",
      " 3   lon_min  float64\n",
      " 4   lon_max  float64\n",
      " 5   rain     float64\n",
      " 6   model    object \n",
      "dtypes: float64(5), object(2)\n",
      "memory usage: 3.7+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save combined data back to csv\n",
    "df.to_csv(p_combined, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'6.0GB'"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check size of full csv on disk\n",
    "nn.to_human(p_combined.stat().st_size, prec=1, family='filesize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 16101.11 MiB, increment: 0.73 MiB\n",
      "CPU times: user 79.4 ms, sys: 66.1 ms, total: 145 ms\n",
      "Wall time: 1.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# Load csv with dask\n",
    "\n",
    "ddf = dd.read_csv(p_combined, assume_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>lat_min</th>\n      <th>lat_max</th>\n      <th>lon_min</th>\n      <th>lon_max</th>\n      <th>rain</th>\n      <th>model</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1889-01-01 12:00:00</td>\n      <td>-35.439867</td>\n      <td>-33.574619</td>\n      <td>141.5625</td>\n      <td>143.4375</td>\n      <td>4.244226e-13</td>\n      <td>MPI-ESM-1-2-HAM</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1889-01-02 12:00:00</td>\n      <td>-35.439867</td>\n      <td>-33.574619</td>\n      <td>141.5625</td>\n      <td>143.4375</td>\n      <td>4.217326e-13</td>\n      <td>MPI-ESM-1-2-HAM</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1889-01-03 12:00:00</td>\n      <td>-35.439867</td>\n      <td>-33.574619</td>\n      <td>141.5625</td>\n      <td>143.4375</td>\n      <td>4.498125e-13</td>\n      <td>MPI-ESM-1-2-HAM</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1889-01-04 12:00:00</td>\n      <td>-35.439867</td>\n      <td>-33.574619</td>\n      <td>141.5625</td>\n      <td>143.4375</td>\n      <td>4.251282e-13</td>\n      <td>MPI-ESM-1-2-HAM</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1889-01-05 12:00:00</td>\n      <td>-35.439867</td>\n      <td>-33.574619</td>\n      <td>141.5625</td>\n      <td>143.4375</td>\n      <td>4.270161e-13</td>\n      <td>MPI-ESM-1-2-HAM</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                  time    lat_min    lat_max   lon_min   lon_max  \\\n0  1889-01-01 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n1  1889-01-02 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n2  1889-01-03 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n3  1889-01-04 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n4  1889-01-05 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n\n           rain            model  \n0  4.244226e-13  MPI-ESM-1-2-HAM  \n1  4.217326e-13  MPI-ESM-1-2-HAM  \n2  4.498125e-13  MPI-ESM-1-2-HAM  \n3  4.251282e-13  MPI-ESM-1-2-HAM  \n4  4.270161e-13  MPI-ESM-1-2-HAM  "
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Python EDA\n",
    "\n",
    "Here we will investigate and summarize the following approaches to reduce memory usage while performing a simple EDA (find maximum rainfall):\n",
    "\n",
    "1. Baseline\n",
    "2. Load data in chunks\n",
    "3. Load only columns of interest\n",
    "4. Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Baseline\n",
    "\n",
    "Naive approach, read all columns with pandas `read_csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_max_rain = lambda x: print(f'Max rainfall: {x:.2f} mm/day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max rainfall: 432.94 mm/day\n",
      "peak memory: 14973.23 MiB, increment: 2598.79 MiB\n",
      "CPU times: user 54.6 s, sys: 4.27 s, total: 58.9 s\n",
      "Wall time: 59.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "max_rain_baseline = pd.read_csv(p_combined).rain.max()\n",
    "print_max_rain(max_rain_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load data in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max rainfall: 432.94 mm/day\n",
      "peak memory: 1322.71 MiB, increment: 1035.25 MiB\n",
      "CPU times: user 52.1 s, sys: 2.77 s, total: 54.8 s\n",
      "Wall time: 55.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "max_rain_chunks = np.finfo('float64').min\n",
    "\n",
    "for df_chunk in pd.read_csv(p_combined, chunksize=1_000_000):\n",
    "    cur_max = df_chunk.rain.max()\n",
    "    if cur_max > max_rain_chunks:\n",
    "        max_rain_chunks = cur_max\n",
    "\n",
    "print_max_rain(max_rain_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Load only columns of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max rainfall: 432.94 mm/day\n",
      "peak memory: 5452.88 MiB, increment: 890.59 MiB\n",
      "CPU times: user 25.8 s, sys: 1.14 s, total: 27 s\n",
      "Wall time: 27.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "df_one_col = pd.read_csv(p_combined, usecols=['rain'])\n",
    "max_rain_one = df_one_col.rain.max()\n",
    "print_max_rain(max_rain_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max rainfall: 432.94 mm/day\n",
      "peak memory: 5555.89 MiB, increment: 920.82 MiB\n",
      "CPU times: user 1min 15s, sys: 12.1 s, total: 1min 28s\n",
      "Wall time: 24.8 s\n",
      "Max rainfall: 432.94 mm/day\n",
      "peak memory: 5518.82 MiB, increment: 995.95 MiB\n",
      "CPU times: user 1min 16s, sys: 12.5 s, total: 1min 28s\n",
      "Wall time: 25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "ddf = dd.read_csv(p_combined)\n",
    "max_rain_dask = ddf.rain.max().compute()\n",
    "\n",
    "print_max_rain(max_rain_dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "The following table summarizes memory usage and execution time for the simple EDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<style  type=\"text/css\" >\n#T_638e3_row0_col0,#T_638e3_row0_col1{\n            background-color:  #da3b46;\n            color:  #f1f1f1;\n        }#T_638e3_row1_col0,#T_638e3_row3_col1{\n            background-color:  #417ca8;\n            color:  #000000;\n        }#T_638e3_row1_col1{\n            background-color:  #e7999e;\n            color:  #000000;\n        }#T_638e3_row2_col0{\n            background-color:  #afc6d7;\n            color:  #000000;\n        }#T_638e3_row2_col1{\n            background-color:  #5288af;\n            color:  #000000;\n        }#T_638e3_row3_col0{\n            background-color:  #b1c7d7;\n            color:  #000000;\n        }</style><table id=\"T_638e3_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Peak Memory Usage (MB)</th>        <th class=\"col_heading level0 col1\" >Execution Time (S)</th>    </tr>    <tr>        <th class=\"index_name level0\" >Method</th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>    </tr></thead><tbody>\n                <tr>\n                        <th id=\"T_638e3_level0_row0\" class=\"row_heading level0 row0\" >baseline</th>\n                        <td id=\"T_638e3_row0_col0\" class=\"data row0 col0\" >14973</td>\n                        <td id=\"T_638e3_row0_col1\" class=\"data row0 col1\" >65</td>\n            </tr>\n            <tr>\n                        <th id=\"T_638e3_level0_row1\" class=\"row_heading level0 row1\" >chunks</th>\n                        <td id=\"T_638e3_row1_col0\" class=\"data row1 col0\" >1323</td>\n                        <td id=\"T_638e3_row1_col1\" class=\"data row1 col1\" >55</td>\n            </tr>\n            <tr>\n                        <th id=\"T_638e3_level0_row2\" class=\"row_heading level0 row2\" >single_column</th>\n                        <td id=\"T_638e3_row2_col0\" class=\"data row2 col0\" >5452</td>\n                        <td id=\"T_638e3_row2_col1\" class=\"data row2 col1\" >27</td>\n            </tr>\n            <tr>\n                        <th id=\"T_638e3_level0_row3\" class=\"row_heading level0 row3\" >dask</th>\n                        <td id=\"T_638e3_row3_col0\" class=\"data row3 col0\" >5529</td>\n                        <td id=\"T_638e3_row3_col1\" class=\"data row3 col1\" >25</td>\n            </tr>\n    </tbody></table>",
      "text/plain": "<pandas.io.formats.style.Styler at 0x7f7fc041a790>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_results = dict(\n",
    "    baseline=[14973, 65],\n",
    "    chunks=[1323, 55],\n",
    "    single_column=[5452, 27],\n",
    "    dask=[5529, 25])\n",
    "\n",
    "pd.DataFrame \\\n",
    "    .from_dict(\n",
    "        m_results,\n",
    "        orient='index',\n",
    "        columns=['Peak Memory Usage (MB)', 'Execution Time (S)']) \\\n",
    "    .rename_axis('Method') \\\n",
    "    .style.pipe(f.bg, rev=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "- To find the maximum rainfall, in this case we only needed one column in the table, therefor loading all columns was redundant.\n",
    "- Loading data in chunks reduced our execution time slightly, and greatly reduced peak memory usage.\n",
    "- Both loading only a single column and Dask had similar memory usage, with Dask executing slightly faster (25s).\n",
    "- Overall Dask reduced our memory usage by ~1/3 and execution time by ~60%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. R EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Save data to multiple formats\n",
    "Here we will compare different methods to make the dataframe available in R:\n",
    "\n",
    "1. Feather\n",
    "2. Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv and create arrow table\n",
    "dataset = ds.dataset(p_combined, format='csv')\n",
    "arrow_table = dataset.to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 9417.54 MiB, increment: 0.36 MiB\n",
      "CPU times: user 3.24 s, sys: 5.83 s, total: 9.07 s\n",
      "Wall time: 5.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "# save feather file\n",
    "\n",
    "p_feather = p_data / 'data.feather'\n",
    "feather.write_feather(arrow_table, p_feather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 12929.43 MiB, increment: 132.63 MiB\n",
      "CPU times: user 8.69 s, sys: 1.48 s, total: 10.2 s\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "# save parquet file\n",
    "# NOTE parquet saves the file in a directory, need to get file path after\n",
    "\n",
    "p_parquet_dir = p_data / 'parquet'\n",
    "pq.write_to_dataset(arrow_table, p_parquet_dir)\n",
    "\n",
    "# get parquet file path\n",
    "p_parquet = list(p_parquet_dir.glob('*'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feather: 1.1GB\n",
      "parquet: 566.1MB\n"
     ]
    }
   ],
   "source": [
    "# check filesizes on disk\n",
    "m_results = defaultdict(dict)\n",
    "m_files = dict(\n",
    "    feather=p_feather,\n",
    "    parquet=p_parquet)\n",
    "\n",
    "for name, p in m_files.items():\n",
    "    size = p.stat().st_size\n",
    "    size_human = nn.to_human(size, prec=1, family='filesize')\n",
    "\n",
    "    m = m_results[name]\n",
    "    m['size_disk'] = size\n",
    "\n",
    "    print(f'{name}: {size_human}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Load data and perform EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "library(arrow)\n",
    "library(dplyr)\n",
    "\n",
    "# set filepaths in R\n",
    "p_feather = \"data/data.feather\"\n",
    "p_parquet = Sys.glob(\"data/parquet/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.61 s, sys: 22.2 s, total: 30.8 s\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "\n",
    "# load feather\n",
    "df_feather <- arrow::read_feather(p_feather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.9 s, sys: 7.23 s, total: 17.1 s\n",
      "Wall time: 8.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "\n",
    "# load parquet\n",
    "df_parquet <- arrow::read_parquet(p_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n",
      "[1] NA\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "print(class(df_feather))\n",
    "\n",
    "max_rainfall <- df_feather %>% select('rain') %>% max()\n",
    "\n",
    "print(max_rainfall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<style  type=\"text/css\" >\n#T_17f56_row0_col0,#T_17f56_row0_col3,#T_17f56_row0_col4,#T_17f56_row1_col1,#T_17f56_row1_col2{\n            background-color:  #417ca8;\n            color:  #000000;\n        }#T_17f56_row0_col1,#T_17f56_row0_col2,#T_17f56_row1_col0,#T_17f56_row1_col3,#T_17f56_row1_col4{\n            background-color:  #da3b46;\n            color:  #f1f1f1;\n        }</style><table id=\"T_17f56_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >peak_memory</th>        <th class=\"col_heading level0 col1\" >size_disk</th>        <th class=\"col_heading level0 col2\" >time_load</th>        <th class=\"col_heading level0 col3\" >time_write</th>        <th class=\"col_heading level0 col4\" >time_total</th>    </tr>    <tr>        <th class=\"index_name level0\" >Method</th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>    </tr></thead><tbody>\n                <tr>\n                        <th id=\"T_17f56_level0_row0\" class=\"row_heading level0 row0\" >feather</th>\n                        <td id=\"T_17f56_row0_col0\" class=\"data row0 col0\" >9,417</td>\n                        <td id=\"T_17f56_row0_col1\" class=\"data row0 col1\" >1,096,622,306</td>\n                        <td id=\"T_17f56_row0_col2\" class=\"data row0 col2\" >12.40</td>\n                        <td id=\"T_17f56_row0_col3\" class=\"data row0 col3\" >5.82</td>\n                        <td id=\"T_17f56_row0_col4\" class=\"data row0 col4\" >18.22</td>\n            </tr>\n            <tr>\n                        <th id=\"T_17f56_level0_row1\" class=\"row_heading level0 row1\" >parquet</th>\n                        <td id=\"T_17f56_row1_col0\" class=\"data row1 col0\" >12,929</td>\n                        <td id=\"T_17f56_row1_col1\" class=\"data row1 col1\" >566,053,909</td>\n                        <td id=\"T_17f56_row1_col2\" class=\"data row1 col2\" >8.90</td>\n                        <td id=\"T_17f56_row1_col3\" class=\"data row1 col3\" >10.90</td>\n                        <td id=\"T_17f56_row1_col4\" class=\"data row1 col4\" >19.80</td>\n            </tr>\n    </tbody></table>",
      "text/plain": "<pandas.io.formats.style.Styler at 0x7f7ed16e1e80>"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare results metrics\n",
    "\n",
    "m_feather = dict(\n",
    "    peak_memory=9417,\n",
    "    time_write=5.82,\n",
    "    time_load=12.4)\n",
    "\n",
    "m_parquet = dict(\n",
    "    peak_memory=12929,\n",
    "    time_write=10.9,\n",
    "    time_load=8.9)\n",
    "    \n",
    "m_results['feather'].update(m_feather)\n",
    "m_results['parquet'].update(m_parquet)\n",
    "\n",
    "pd.DataFrame \\\n",
    "    .from_dict(m_results).T \\\n",
    "    .rename_axis('Method') \\\n",
    "    .sort_index(axis=1) \\\n",
    "    .assign(time_total=lambda x: x.time_write + x.time_load) \\\n",
    "    .style \\\n",
    "    .pipe(f.bg, rev=False) \\\n",
    "    .format('{:,.0f}') \\\n",
    "    .format('{:.2f}', subset=['time_write', 'time_load', 'time_total'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our team chose to use the `Feather` file format to transfer data between python and R over `Parquet`.\n",
    "\n",
    "Both file types had similar write, load, and total times to transfer data, and parquet even used half the memory on disk. However, the parquet file type is a bit more involved to deal with (have to handle more complex file paths), as it is primarily mean for long term storage. If we were optimizing for storage size rather than ease of completing a simple EDA, we would have chosen parquet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('525': conda)",
   "name": "python386jvsc74a57bd0853d569a9b7da01add2e2d9599a0bacc0dbbc936a6f6feab1c78827b3cfdc6dd"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "853d569a9b7da01add2e2d9599a0bacc0dbbc936a6f6feab1c78827b3cfdc6dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}